```@meta
EditURL = "../../../literate/tutorials/Autoencoder_EEG_Meeting_Minutes.jl"
```

Things to learn:

1. What is an autoencoder?
2. What is a deep autoencoder.
3. What is LSTM.

Questions to be asked to the professor:

1. Do we have to build an autoencoder from scratch?
2. What do you mean by improvement?
3. Do we have to inculcate LSTM into our Autoencoder

1.Learning rate

- Too high a learning rate might cause the model to oscillate or even diverge during training, leading to poor convergence. This could result in poor quality of the graphics generated by the autoencoder, with significant loss of detail.
- Too low a learning rate might lead to excessively slow training and the model might get stuck in local minima. This could cause the autoencoder to generate overly smooth images that miss some important features.

2. Epoch Number

- Too few epochs might prevent the model from adequately learning the features in the data, affecting the quality and accuracy of the reconstructed graphics.
- Too many epochs might lead to overfitting where the model performs well on the training data but poorly on new, unseen data. An overfitted autoencoder might generate images that are too reliant on specific noise and details of the training data, instead of learning a more general representation of the data.

3. Batch Size

- Smaller batch sizes generally provide a more accurate gradient estimation but might lead to a less stable training process and longer training times. Smaller batches might enable the model to learn more details, potentially leading to better performance in image reconstruction tasks.
- Larger batch sizes can speed up training and stabilize gradient estimations but may reduce the generalization ability of the model during training. In autoencoders, too large a batch might result in reconstructed images that lack detail and appear more blurred or smooth.

---

*This page was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*

